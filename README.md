# ğŸ—„ï¸ plateforme de scoring fraude chargement vers Snowflake: 

Pipelines fiables pour lâ€™ingestion et la transformation des donnÃ©es

## ğŸ¯ Objectif
Mettre en place des pipelines **batch**, **streaming**, et de **qualitÃ© de donnÃ©es** afin dâ€™assurer la fiabilitÃ©, la cohÃ©rence et la fraÃ®cheur des donnÃ©es pour les analyses.

---
## ğŸ”„ Pipeline global

```mermaid
flowchart TD
    %% =====================================
    %% ğŸ—ï¸ Pipeline Data Engineering vertical
    %% =====================================

    %% Niveau 1 : ETL
    subgraph ETL[" ETL & Data Pipeline"]
        A[" Extraction CRM et Transactions J-1"]
        B[" Transformation PySpark"]
        C[" Chargement dans Snowflake ou BigQuery"]
    end

    %% Niveau 2 : Streaming & Features
    subgraph STREAM[" Pipeline Streaming & Feature Engineering"]
        D[" Pipeline Kafka Streaming"]
        E[" Feature Engineering (moyennes glissantes, anomalies)"]
        F[" Sink vers Snowflake ou BigQuery"]
    end

    %% Niveau 3 : QualitÃ© & Monitoring
    subgraph QUALITY[" ContrÃ´le QualitÃ© & Monitoring"]
        G[" Great Expectations : validation des donnÃ©es"]
        H[" Rapport qualitÃ© hebdomadaire"]
        I[" Notification Slack/Email"]
    end

    %% Niveau 4 : Ops / CI-CD
    subgraph OPS[" Orchestration & CI/CD"]
        J[" Airflow DAGs : orchestration ETL/Streaming"]
        K[" GitHub Actions : tests et dÃ©ploiement automatique"]
        L[" Monitoring : logs, alertes, dashboards"]
    end

    %% ğŸ”— Flux hiÃ©rarchique vertical
    ETL --> STREAM --> QUALITY --> OPS
    A --> B --> C --> D --> E --> F --> G --> H --> I
    J --> ETL
    J --> STREAM
    K --> ETL
    K --> STREAM
    L --> QUALITY


```
---

## ğŸ§© Structure du Projet

```bash
fraud_scoring_platform/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag_batch_customers_transactions.py      # DAG Airflow pour le pipeline batch (extraction CRM + transactions J-1)
â”‚   â”œâ”€â”€ dag_streaming_transactions.py            # DAG Airflow pour le pipeline streaming (transactions en temps rÃ©el)
â”‚   â””â”€â”€ dag_data_quality.py                      # DAG Airflow pour la validation et le reporting de la qualitÃ©
â”‚
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”œâ”€â”€ extraction.py                        # Extraction des donnÃ©es CRM et transactions
â”‚   â”‚   â”œâ”€â”€ transform_pyspark.py                 # Nettoyage et transformation PySpark
â”‚   â”‚   â””â”€â”€ load_warehouse.py                    # Chargement dans Snowflake / BigQuery
â”‚   â”‚
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ streaming_job.py                     # Job Spark Structured Streaming avec Kafka
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py               # Calcul des moyennes glissantes et dÃ©tection dâ€™anomalies
â”‚   â”‚   â””â”€â”€ sink_snowflake.py                    # Ã‰criture vers Snowflake / BigQuery
â”‚   â”‚
â”‚   â””â”€â”€ quality/
â”‚       â”œâ”€â”€ great_expectations_checks.py         # DÃ©finition des rÃ¨gles Great Expectations
â”‚       â”œâ”€â”€ generate_quality_report.py           # GÃ©nÃ©ration du rapport de qualitÃ© hebdomadaire
â”‚       â””â”€â”€ notify_quality_status.py             # Notification (Slack/Email)
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ airflow_config.yaml                      # Configuration Airflow (connexions, scheduling)
â”‚   â”œâ”€â”€ spark_config.yaml                        # Configuration Spark (batch et streaming)
â”‚   â”œâ”€â”€ warehouse_config.yaml                    # Connexions Snowflake/BigQuery
â”‚   â””â”€â”€ quality_rules.yaml                       # RÃ¨gles de validation de la qualitÃ© des donnÃ©es
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py                                # Gestion des logs centralisÃ©s
â”‚   â”œâ”€â”€ monitoring.py                            # Suivi de la latence et mÃ©triques
â”‚   â”œâ”€â”€ schema_validation.py                     # Validation de schÃ©ma Avro/Parquet
â”‚   â””â”€â”€ helpers.py                               # Fonctions utilitaires gÃ©nÃ©riques
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ data_quality_report.html                 # Rapport gÃ©nÃ©rÃ© automatiquement par Great Expectations
â”‚   â””â”€â”€ metrics_dashboard.csv                    # Indicateurs de qualitÃ© et volumÃ©trie
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_batch_pipeline.py                   # Tests unitaires du pipeline batch
â”‚   â”œâ”€â”€ test_streaming_pipeline.py               # Tests du pipeline temps rÃ©el
â”‚   â””â”€â”€ test_data_quality.py                     # Tests des rÃ¨gles de validation
â”‚
â”œâ”€â”€ Dockerfile                                   # Image Airflow + Spark + GE
â”œâ”€â”€ requirements.txt                             # DÃ©pendances Python
â””â”€â”€ README.md
```
---
## âš™ï¸ Stack Technique
- **Orchestration** : Apache Airflow
- **Traitement batch** : PySpark
- **Traitement temps rÃ©el** : Spark Structured Streaming + Kafka
- **Stockage analytique** : Snowflake / BigQuery
- **QualitÃ© des donnÃ©es** : Great Expectations
- **Monitoring** : Airflow + Logs + Metrics

## ğŸš€ ExÃ©cution

```bash
# Lancer Airflow
docker-compose up airflow-webserver airflow-scheduler

# DÃ©clencher le DAG batch manuellement
airflow dags trigger dag_batch_customers_transactions

# Visualiser le rapport de qualitÃ©
open reports/data_quality_report.html

```