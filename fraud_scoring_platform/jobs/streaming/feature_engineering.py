# Calcul des moyennes glissantes et dÃ©tection dâ€™anomalies

# ---------------------------------------------------
# Objectif : calculer des moyennes glissantes et dÃ©tecter des anomalies
# Ã  partir des donnÃ©es de transactions nettoyÃ©es.
# Compatible avec Spark Batch ou Streaming.
# ---------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, avg, stddev, count, when, window
)
from pyspark.sql import DataFrame


def create_spark_session(app_name="FeatureEngineering"):
    """
    CrÃ©e une session Spark.
    """
    spark = (
        SparkSession.builder
        .appName(app_name)
        .config("spark.sql.shuffle.partitions", "4")
        .getOrCreate()
    )
    return spark


def compute_features(df: DataFrame, time_window="10 minutes", slide="5 minutes") -> DataFrame:
    """
    Calcule les moyennes glissantes, Ã©cart-types et anomalies.
    
    Args:
        df (DataFrame): donnÃ©es contenant 'customer_id', 'amount', 'timestamp'
        time_window (str): taille de la fenÃªtre glissante
        slide (str): intervalle de glissement
    
    Returns:
        DataFrame enrichi avec :
          - rolling_avg_amount
          - stddev_amount
          - transaction_count
          - anomaly_flag
    """
    windowed_df = (
        df.groupBy(
            window(col("timestamp"), time_window, slide),
            col("customer_id")
        )
        .agg(
            avg("amount").alias("rolling_avg_amount"),
            stddev("amount").alias("stddev_amount"),
            count("*").alias("transaction_count")
        )
    )

    # DÃ©tection simple dâ€™anomalies :
    # - Montant moyen trop Ã©levÃ©
    # - Volume de transactions anormalement Ã©levÃ©
    # - Ecart-type trÃ¨s Ã©levÃ©
    enriched_df = windowed_df.withColumn(
        "anomaly_flag",
        when(
            (col("rolling_avg_amount") > 10000) |
            (col("transaction_count") > 50) |
            (col("stddev_amount") > 5000),
            1
        ).otherwise(0)
    )

    return enriched_df


def main(input_path="data/processed/transactions_cleaned/", output_path="data/processed/transactions_features/"):
    """
    Script exÃ©cutable pour calculer les features sur les donnÃ©es batch.
    """
    spark = create_spark_session()

    # Lecture des donnÃ©es nettoyÃ©es
    df = spark.read.parquet(input_path)

    print(f"ğŸ“Š DonnÃ©es chargÃ©es depuis {input_path}")
    print(f"Nombre d'enregistrements : {df.count()}")

    # Calcul des features
    features_df = compute_features(df)

    # Ã‰criture du rÃ©sultat
    (
        features_df.write
        .mode("overwrite")
        .parquet(output_path)
    )

    print(f"âœ… Features calculÃ©es et enregistrÃ©es dans : {output_path}")

    spark.stop()


if __name__ == "__main__":
    import sys
    input_path = sys.argv[1] if len(sys.argv) > 1 else "data/processed/transactions_cleaned/"
    output_path = sys.argv[2] if len(sys.argv) > 2 else "data/processed/transactions_features/"
    main(input_path, output_path)
