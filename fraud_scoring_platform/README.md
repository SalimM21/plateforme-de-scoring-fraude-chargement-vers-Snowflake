# Documentation du projet Fraud Scoring Platform (Module 2)
---
## ğŸš€ Data Platform â€“ Pipelines Batch, Streaming et QualitÃ© des DonnÃ©es

Ce projet implÃ©mente une plateforme complÃ¨te de pipelines **Airflow + Spark + Great Expectations**, intÃ©grant :
- Traitement **batch** (donnÃ©es CRM + transactions J-1)
- Traitement **streaming** en temps rÃ©el (Kafka â†’ Snowflake/BigQuery)
- Validation et **monitoring de la qualitÃ© des donnÃ©es**

---

## ğŸ“‚ Structure du Projet

```bash
data_pipeline_project/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag_batch_customers_transactions.py
â”‚   â”œâ”€â”€ dag_streaming_transactions.py
â”‚   â””â”€â”€ dag_data_quality.py
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ extraction.py
â”‚   â”œâ”€â”€ transform_pyspark.py
â”‚   â”œâ”€â”€ load_warehouse.py
â”‚   â”œâ”€â”€ streaming_job.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ sink_snowflake.py
â”‚   â”œâ”€â”€ great_expectations_checks.py
â”‚   â”œâ”€â”€ generate_quality_report.py
â”‚   â””â”€â”€ notify_quality_status.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ airflow_config.yaml
â”‚   â”œâ”€â”€ spark_config.yaml
â”‚   â”œâ”€â”€ warehouse_config.yaml
â”‚   â””â”€â”€ quality_rules.yaml
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ monitoring.py
â”‚   â”œâ”€â”€ schema_validation.py
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_batch_pipeline.py
â”‚   â”œâ”€â”€ test_streaming_pipeline.py
â”‚   â””â”€â”€ test_data_quality.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation et Lancement

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/your-org/data-pipeline-project.git
cd data-pipeline-project
```

### 2ï¸âƒ£ Construire et lancer les conteneurs

```bash
docker-compose up --build
```

### 3ï¸âƒ£ DÃ©marrer Airflow

Airflow sera disponible Ã  lâ€™adresse :  
ğŸ‘‰ `http://localhost:8080`  
Login par dÃ©faut : `airflow / airflow`

### 4ï¸âƒ£ Lancer les DAGs

Dans lâ€™interface Airflow :
- `dag_batch_customers_transactions` : pipeline batch quotidien  
- `dag_streaming_transactions` : pipeline streaming Kafka  
- `dag_data_quality` : vÃ©rification et rapport qualitÃ© hebdomadaire  

### 5ï¸âƒ£ VÃ©rifier les logs et mÃ©triques

Logs disponibles dans `/logs/`  
MÃ©triques consultables via **Prometheus / Grafana**

---

## ğŸ“˜ Documentation Technique

### ğŸ”¹ Pipelines

| Type | Description | Techno |
|------|--------------|--------|
| Batch | Extraction et transformation PySpark | Airflow + Spark |
| Streaming | Transactions Kafka + Feature Engineering | Spark Structured Streaming |
| Data Quality | RÃ¨gles Great Expectations + Rapport hebdo | GE + Airflow |

### ğŸ”¹ Configurations

- `configs/airflow_config.yaml` : planification et connexions Airflow  
- `configs/spark_config.yaml` : paramÃ¨tres Spark (batch + streaming)  
- `configs/warehouse_config.yaml` : credentials Snowflake / BigQuery  
- `configs/quality_rules.yaml` : rÃ¨gles de validation des donnÃ©es  

---

## ğŸ§ª Tests

ExÃ©cution des tests unitaires :

```bash
pytest tests/ --disable-warnings -v
```

---

## ğŸ§± Dockerfile (extrait)

```Dockerfile
FROM apache/airflow:2.9.0-python3.9

RUN pip install --no-cache-dir -r requirements.txt
COPY . /opt/airflow/
ENV PYTHONPATH=/opt/airflow
```

---

## ğŸ“ˆ Monitoring

- **Latence streaming** mesurÃ©e via `monitoring.py`
- **Logs centralisÃ©s** dans `/logs` avec rotation automatique
- **Alertes Slack/Email** via `notify_quality_status.py`

---

## ğŸ“Š Exemple de Diagramme Mermaid

```mermaid
graph TD
  A[Kafka Transactions] --> B[Spark Structured Streaming]
  B --> C["Feature Engineering: moyenne glissante, anomalies"]
  C --> D["Sink Snowflake/BigQuery"]
  E[CRM Extract Batch] --> F[PySpark Transform]
  F --> D
  D --> G["Great Expectations Validation"]
  G --> H["Quality Report + Slack Notify"]
```

---

## ğŸ‘¨â€ğŸ’» Auteurs

- **Data Engineer** : Salim Majide  
- **Stack utilisÃ©e** : Airflow, Spark, Kafka, Snowflake, Great Expectations

---

## ğŸ Commandes rapides

```bash
# DÃ©marrer tout
docker-compose up -d

# Lancer un DAG spÃ©cifique
airflow dags trigger dag_batch_customers_transactions

# VÃ©rifier les logs
docker logs airflow-scheduler

# ExÃ©cuter les tests
pytest tests/
```

---

## ğŸ“š RÃ©fÃ©rences

- [Apache Airflow](https://airflow.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [Great Expectations](https://greatexpectations.io/)
- [Snowflake Documentation](https://docs.snowflake.com/)
- [Google BigQuery](https://cloud.google.com/bigquery)